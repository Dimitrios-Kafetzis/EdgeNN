% =============================================================================
% Section 3: Design Principles (~1 page)
% =============================================================================

\section{Design Principles}
\label{sec:design_principles}

The design of EdgeNN is guided by five principles, each motivated by a
specific limitation of existing MCU inference solutions.  These principles
inform every architectural decision described in
Sections~\ref{sec:architecture}--\ref{sec:quantization}.

% ---------------------------------------------------------------------------
\subsection{P1: Zero-Allocation Inference}
\label{sec:p1_zero_alloc}

\textbf{Problem.}
Dynamic memory allocation on Cortex-M processors introduces two categories
of non-determinism.  First, calls to \texttt{malloc} and \texttt{free}
exhibit variable latency---typically 15--40\,$\mu$s per call on Cortex-M7 at
480\,MHz, depending on heap fragmentation state.  Second, repeated
allocate-free cycles create fragmentation that can cause allocation failures
even when total free memory exceeds the request size, a failure mode that is
unacceptable in safety-critical systems.

\textbf{Solution.}
EdgeNN eliminates all dynamic allocation from the inference path.  Memory is
managed through static arena allocators (Section~\ref{sec:arch_memory}) that
use a bump pointer for $O(1)$ allocation with zero fragmentation.  All buffer
sizes are determined at model load time through a dry-run memory planning pass
that computes the peak scratch requirement across all layers.  During inference,
no allocation occurs---only pointer arithmetic within pre-allocated regions.

\textbf{Benefit.}
Cycle count variance between successive inferences is bounded to less than
1\%, determined solely by data-dependent branching in activation functions
(e.g., ReLU clamping).  This determinism enables deployment in systems subject
to ISO~26262 and IEC~61508 certification requirements.

% ---------------------------------------------------------------------------
\subsection{P2: Tensor Descriptor, Not Object}
\label{sec:p2_tensor_descriptor}

\textbf{Problem.}
Object-oriented tensor abstractions, such as TFLite~Micro's
\texttt{TfLiteTensor}, bundle data pointers, shape metadata, allocation
state, and type information into structures that carry significant per-tensor
overhead and require indirection through accessor functions.

\textbf{Solution.}
An EdgeNN tensor is a lightweight descriptor of approximately 80~bytes that
contains a raw data pointer, shape array, stride array, data type, memory
layout, and quantization parameters.  Critically, the descriptor does not own
its data: the \texttt{data} pointer may reference arena-allocated SRAM,
memory-mapped Flash (for weights), or a user-supplied buffer.  No reference
counting, no copy-on-write, no virtual dispatch.

\textbf{Benefit.}
Operators receive tensor descriptors by pointer and work directly on the
underlying data without indirection overhead.  Tensor descriptors can reside
on the stack (no arena allocation required for metadata), and the absence of
ownership semantics eliminates an entire class of memory management bugs.

% ---------------------------------------------------------------------------
\subsection{P3: Quantization-First Design}
\label{sec:p3_quantization_first}

\textbf{Problem.}
Libraries designed around FP32 computation that add quantization as an
afterthought produce suboptimal INT8 paths.  The quantized kernels must
interoperate with infrastructure originally designed for floating-point tensors,
leading to unnecessary type conversions and metadata overhead.

\textbf{Solution.}
In EdgeNN, INT8 symmetric quantization is the \emph{primary} execution path;
FP32 serves as the reference fallback for validation and for targets with
fast floating-point units (e.g., Cortex-A with NEON).  Weights use per-channel
quantization with zero point fixed at zero.  The requantization from INT32
accumulators to INT8 output is performed via a precomputed fixed-point
multiplier--shift pair (Section~\ref{sec:quant_requantize}), eliminating all
floating-point operations from the inference path.  For accumulation-sensitive
operations---LSTM cell state propagation and attention score
computation---INT16 precision is selectively employed.

\textbf{Benefit.}
The INT8 path requires no FPU, enabling deployment on Cortex-M0+ and Cortex-M4
cores without single-precision hardware.  Fixed-point requantization executes
in 2~cycles on Cortex-M7 (one multiply, one shift), compared to approximately
14~cycles for an equivalent floating-point scale-and-round sequence.

% ---------------------------------------------------------------------------
\subsection{P4: Layered HAL with Generic Fallback}
\label{sec:p4_layered_hal}

\textbf{Problem.}
ARM-optimized intrinsics (CMSIS-NN, Helium/MVE, NEON) deliver 2--8$\times$
speedup over scalar C but are not portable across ISA variants.  Libraries that
require a specific backend cannot be developed or tested on host workstations.

\textbf{Solution.}
Every operator in EdgeNN has a generic C implementation that compiles and
produces correct results on any platform with a C11 compiler.  Optimized
backends are selected at compile time via \texttt{\#ifdef} guards controlled by
CMake options (\texttt{EDGENN\_USE\_CMSIS\_NN}, \texttt{EDGENN\_USE\_HELIUM},
\texttt{EDGENN\_USE\_NEON}).  The HAL layer additionally abstracts
platform-specific facilities such as the DWT cycle counter (Cortex-M) and
\texttt{clock\_gettime} (POSIX hosts).

\textbf{Benefit.}
Developers build and run the full test suite on x86-64 or ARM64 workstations,
then cross-compile for the target MCU with optimized backends enabled.  The
generic C path also serves as a reference for validating optimized
implementations.

% ---------------------------------------------------------------------------
\subsection{P5: Modular Architecture with Dead Code Elimination}
\label{sec:p5_modular}

\textbf{Problem.}
TFLite~Micro's interpreter-based architecture pulls in the entire operator
registration table regardless of which operators a given model uses, resulting
in approximately 100\,KB of Flash for the runtime alone.

\textbf{Solution.}
EdgeNN is structured as a static library of independent translation units, one
per operator, with no global registration table.  The graph runtime dispatches
operators via a \texttt{switch} statement on the operator type enumeration.
Combined with link-time optimization (\texttt{-flto}) and section-based garbage
collection (\texttt{-ffunction-sections -fdata-sections -Wl,--gc-sections}),
the linker eliminates all operator code not reachable from the application's
call graph.

\textbf{Benefit.}
A model using only Dense and ReLU operators links against approximately
8\,KB of library code.  A full DNN model (Dense, Conv2D, Depthwise Conv2D,
Pooling, Batch Normalization, activations) requires under 20\,KB.  The
complete library with all 18 operators compiles to under 50\,KB of Flash.

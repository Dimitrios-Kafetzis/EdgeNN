% =============================================================================
% Section 1: Introduction (~1.5 pages)
% =============================================================================

\section{Introduction}
\label{sec:introduction}

The deployment of neural network models on microcontrollers (MCUs) has emerged
as a critical capability for applications that demand low-latency inference,
data privacy, and operation in bandwidth-constrained environments.  Keyword
spotting on wearable devices, predictive maintenance in industrial plants,
anomaly detection in autonomous maritime systems, and sensor fusion in
unmanned aerial vehicles all require inference to execute locally on processors
with as little as 64\,KB of SRAM and 256\,KB of Flash.  These
resource-constrained platforms---predominantly based on ARM Cortex-M4, M7, and
M33 cores---lack memory management units, run bare-metal or under minimal
real-time operating systems, and must deliver bounded, deterministic execution
times.  In safety-critical domains governed by standards such as
ISO~26262 (automotive), IEC~61508 (industrial), and SOLAS (maritime), the
inference engine must guarantee not only correctness but also cycle-count
reproducibility, freedom from dynamic memory allocation, and absence of
undefined behavior.

Several frameworks address neural network inference on microcontrollers, yet
each imposes significant constraints.  TensorFlow Lite Micro
(TFLite~Micro)~\cite{david2021tensorflow} is the most widely adopted solution,
providing an interpreter-based runtime with broad operator coverage for
convolutional and dense architectures.  However, its C++ implementation incurs
approximately 100\,KB of Flash for the interpreter runtime alone, employs
non-deterministic memory allocation patterns through its memory planner, and
offers limited support for recurrent (LSTM, GRU) and Transformer (Multi-Head
Attention) operators.  CMSIS-NN~\cite{lai2018cmsis}, developed by Arm, provides
highly optimized INT8 operator kernels for Cortex-M processors that achieve
near-peak throughput on DSP-enabled cores.  Yet CMSIS-NN operates exclusively
at the operator level: it provides no graph runtime, no model loading
mechanism, and no support for RNN or Transformer architectures, requiring
substantial integration effort to assemble into a complete inference pipeline.
The Apache TVM project~\cite{chen2018tvm} offers microTVM, a compiler-based
approach that generates target-specific C code, but it requires LLVM
infrastructure, introduces a complex deployment pipeline, and is not
well-suited for bare-metal environments without operating system support.
Vendor-specific tools such as STMicroelectronics' X-CUBE-AI are
proprietary, closed-source, and locked to a single hardware family.  Academic
efforts including TinyEngine~\cite{lin2021mcunetv2} and
MCUNet~\cite{lin2020mcunet} have demonstrated that co-designing model
architecture and inference engine yields significant efficiency gains, but
these focus primarily on convolutional architectures and do not provide a
general-purpose library supporting recurrent or attention-based models.

A gap therefore exists in the landscape of MCU inference tools.  There is no
lightweight, self-contained C library that simultaneously provides (a)~zero-allocation,
deterministic inference with bounded execution time, (b)~full operator coverage
spanning Dense, Convolutional, Recurrent, and Transformer architectures,
(c)~a quantization-first INT8 design with per-channel scaling and INT16
precision for accumulation-sensitive operations, and (d)~a modular graph
runtime whose design enables model partitioning across multiple resource-constrained
devices---all within a Flash footprint under 50\,KB suitable for safety-critical
embedded deployment.

This paper presents \textbf{EdgeNN}, a pure C11 library designed from the ground
up for deterministic neural network inference on ARM Cortex-M and Cortex-A
processors.  The principal contributions of this work are as follows:

\begin{enumerate}[leftmargin=*]

\item A \textbf{zero-allocation inference engine} written in pure C11 with no
external dependencies.  All memory is managed through static arena allocators
with $O(1)$ bump-pointer allocation and zero fragmentation.  The inference path
contains no calls to \texttt{malloc}, \texttt{free}, or any heap allocator,
yielding deterministic execution suitable for safety-critical deployment.

\item A \textbf{quantization-first design} in which INT8 symmetric quantization
is the primary execution path, not a retrofit.  Weights use per-channel
quantization with zero point fixed at zero; activations use per-tensor scaling.
Accumulation is performed in INT32, and requantization is implemented via
precomputed fixed-point multiplier--shift pairs that eliminate all
floating-point operations from the inference path.  INT16 precision is
selectively employed for accumulation-sensitive operations, including LSTM cell
state propagation and attention score computation.

\item \textbf{Full operator coverage across three architecture families}:
18 operators spanning DNN (Dense, Conv2D, Depthwise Conv2D, Pooling, Batch
Normalization, six activation functions), RNN (LSTM cell and sequence, GRU cell
and sequence, Simple RNN), and Transformer (Multi-Head Attention with optional
KV cache, Layer Normalization, Feed-Forward Network, Positional Encoding)
architectures.  To the best of our knowledge, EdgeNN is the first C library to
unify these three families under a single zero-allocation inference framework.

\item A \textbf{layered hardware abstraction} with a portable generic~C backend
that compiles on any platform with a C11 compiler, and compile-time selectable
optimized backends for CMSIS-NN (Cortex-M), Helium/MVE (Cortex-M55), and NEON
(Cortex-A53/A72).  This separation enables development and testing on host
workstations while deploying optimized code on target hardware.

\item A \textbf{static graph runtime} with a binary model format that supports
zero-copy weight mapping from Flash, automatic scratch arena management via a
ping-pong buffer scheme that reduces peak SRAM usage by approximately 50\%
compared to na\"ive separate allocation, and compile-time memory planning that
determines the exact buffer sizes required before inference begins.

\item A \textbf{partitioning-aware architecture} that enables model splitting
at arbitrary layer boundaries for distributed inference across multiple
MCUs~\cite{kafetzis2025wiopt}.  Tensor descriptors carry complete quantization
metadata, ensuring zero ambiguity at partition boundaries, and each partition
manages its own scratch arena independently, enabling communication of
intermediate tensors over SPI, UART, or network interfaces.

\end{enumerate}

EdgeNN is validated by 118~unit tests covering all operators in both FP32 and
INT8 execution paths, with quantized outputs verified against FP32 reference
implementations within one quantization step of accuracy.  The library is
open-source under the MIT License and available at
\url{https://github.com/Dimitrios-Kafetzis/EdgeNN}.

The remainder of this paper is organized as follows.
Section~\ref{sec:related_work} reviews related work in MCU inference
frameworks, quantization techniques, and model partitioning.
Section~\ref{sec:design_principles} presents the five design principles that
guide EdgeNN's architecture.  Section~\ref{sec:architecture} describes the
system architecture, including the arena memory manager, tensor descriptor, and
graph runtime.  Section~\ref{sec:operators} details the operator
implementations across DNN, RNN, and Transformer families.
Section~\ref{sec:quantization} explains the quantization pipeline, including
fixed-point requantization and LUT-based activation functions.
Section~\ref{sec:evaluation} presents the experimental evaluation.
Section~\ref{sec:partitioning} discusses model partitioning support and its
connection to distributed edge inference research.  Finally,
Section~\ref{sec:conclusion} concludes with a summary and directions for future
work.

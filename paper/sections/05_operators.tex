% =============================================================================
% Section 5: Operator Implementation (~2 pages)
% =============================================================================

\section{Operator Implementation}
\label{sec:operators}

% ---------------------------------------------------------------------------
\subsection{DNN Operators}
\label{sec:ops_dnn}

% Dense: INT8 matmul with INT32 accumulation, per-channel requantization,
%        fused activation.
% Conv2D: im2col + GEMM strategy, 1x1 pointwise optimization, SAME/VALID.
% Depthwise Conv2D: channel-wise direct convolution (no im2col).
% Pooling: MaxPool (dtype-preserving), AvgPool (with requantization).
% Activations: ReLU/ReLU6 as clamping, Sigmoid/Tanh/GELU via 256-entry LUTs.
%
% TODO: Write DNN operators subsection
% TODO: Reference operator_support table

% ---------------------------------------------------------------------------
\subsection{RNN Operators}
\label{sec:ops_rnn}

% LSTM: Fused gate computation (single matmul for all 4 gates),
%       INT8/INT16 hybrid (gates INT8, cell state INT16).
% GRU: 3-gate with reset-gate-in-hidden-path optimization.
% Sequence unrolling: state carry-over, bidirectional wrapper.
%
% TODO: Write RNN operators subsection

% ---------------------------------------------------------------------------
\subsection{Transformer Operators}
\label{sec:ops_transformer}

% Multi-Head Attention: Q/K/V projection, per-head scaled dot-product,
%   INT16 score accumulation, softmax in fixed-point, optional KV-cache.
% Layer Normalization: INT32 accumulation for mean/variance,
%   fixed-point inverse sqrt.
% FFN: Two Dense layers with GELU activation (LUT-based).
% Positional Encoding: Precomputed sinusoidal tables, RoPE planned.
%
% TODO: Write Transformer operators subsection

% ---------------------------------------------------------------------------
\subsection{Key Implementation Details}
\label{sec:ops_details}

% LUT generation: building 256-entry lookup tables for nonlinear activations.
% Fused operators: Conv+BN+ReLU at weight-folding level.
% INT16 precision decision: INT8 cell state causes >5% accuracy loss in LSTM
%   after 100 time steps, INT16 maintains <0.5% error.
%
% TODO: Write implementation details subsection

% =============================================================================
% Section 2: Related Work (~1.5 pages)
% =============================================================================

\section{Related Work}
\label{sec:related_work}

% ---------------------------------------------------------------------------
\subsection{Inference Frameworks for Microcontrollers}
\label{sec:related_frameworks}

TensorFlow Lite Micro (TFLite~Micro)~\cite{david2021tensorflow} is the most
widely deployed inference framework for microcontrollers.  It employs an
interpreter-based architecture in which a \texttt{MicroInterpreter} dispatches
operators at runtime from a registered kernel table.  While this design
provides flexibility and broad operator coverage for convolutional and dense
architectures, it introduces several limitations for resource-constrained
targets.  The interpreter runtime itself occupies approximately 100\,KB of
Flash, the C++ implementation requires exception handling support and
name-mangled symbols, and the built-in memory planner employs a
greedy algorithm that does not guarantee minimal allocation and can exhibit
non-deterministic allocation patterns across model versions.  Support for
recurrent operators (LSTM, GRU) is partial and limited to specific kernel
registrations, while Transformer operators such as Multi-Head Attention are
absent from the standard kernel set.

CMSIS-NN~\cite{lai2018cmsis}, developed by Arm, takes a fundamentally different
approach by providing hand-optimized operator kernels that exploit the DSP
extensions (SIMD MAC, saturating arithmetic) available on Cortex-M4/M7 cores.
Functions such as \texttt{arm\_fully\_connected\_s8()} and
\texttt{arm\_convolve\_s8()} achieve near-peak throughput for INT8 inference,
and the library footprint is minimal (approximately 5\,KB of Flash for a
typical operator subset).  However, CMSIS-NN operates strictly at the operator
level: it provides no graph runtime, no model loading mechanism, no layer
sequencing logic, and no support for RNN or Transformer architectures.
Integrating CMSIS-NN kernels into a complete inference pipeline requires
substantial application-level code for buffer management, operator scheduling,
and quantization parameter propagation.

The Apache TVM project~\cite{chen2018tvm} offers microTVM, which compiles
models from high-level frameworks into target-specific C code through an LLVM
backend.  This compiler-based approach can produce highly optimized kernels, but
the deployment pipeline requires an LLVM toolchain, a Python-based compilation
host, and runtime support for tensor allocation---making it poorly suited for
bare-metal environments.  Vendor-specific tools, including
STMicroelectronics' X-CUBE-AI and Edge Impulse, provide integrated model
conversion and deployment but are proprietary, locked to specific hardware
families, and offer no source-level access for customization.  Academic projects
such as TinyEngine~\cite{lin2021mcunetv2} have demonstrated that co-designing
the inference engine with the model architecture can yield substantial
efficiency gains, reducing peak memory through techniques such as patch-based
inference.  However, TinyEngine focuses exclusively on convolutional
architectures and is tightly coupled to the MCUNet model family.

% ---------------------------------------------------------------------------
\subsection{Quantization for MCU Inference}
\label{sec:related_quantization}

Quantization reduces the precision of weights and activations from 32-bit
floating-point to lower-bitwidth integer representations, enabling efficient
inference on processors without floating-point units.
Jacob et al.~\cite{jacob2018quantization} established the foundational
quantization scheme adopted by TensorFlow, in which real values are
approximated as $r = S(q - Z)$, where $S$ is a floating-point scale, $Z$ is an
integer zero point, and $q$ is the quantized integer value.  Two principal
approaches exist for determining quantization parameters: post-training
quantization (PTQ), which calibrates scales from a representative dataset after
training, and quantization-aware training (QAT), which simulates quantization
effects during the training loop to recover accuracy.

A key design choice is the quantization granularity.  Per-tensor quantization
assigns a single scale and zero point to an entire tensor, minimizing metadata
overhead but sacrificing accuracy when the dynamic range varies significantly
across output channels.  Per-channel quantization, which assigns independent
parameters to each output channel of a weight tensor, provides substantially
better accuracy for convolutional and dense layers at the cost of per-channel
requantization during inference.  Hybrid precision schemes that combine INT8 for
the majority of computations with INT16 for precision-sensitive operations have
proven valuable for recurrent architectures: LSTM cell state, which
accumulates over potentially hundreds of time steps, exhibits measurable
accuracy degradation when constrained to INT8 precision, while INT16
accumulation maintains error below 0.5\% over typical sequence lengths.

On ARM Cortex-M cores, quantized inference maps naturally to the DSP extension
instructions.  The \texttt{SMLAD} instruction performs dual signed
multiply-accumulate in a single cycle, and the \texttt{SSAT} instruction
provides saturating arithmetic for output clamping.  The requantization step---
converting INT32 accumulators back to INT8---is implemented as a fixed-point
multiply-shift operation: the real scale ratio is decomposed into a normalized
INT32 multiplier in $[2^{30}, 2^{31})$ and an integer right-shift, eliminating
all floating-point operations from the inference path.

% ---------------------------------------------------------------------------
\subsection{Neural Network Architectures on MCUs}
\label{sec:related_architectures}

The dominant architectures deployed on microcontrollers fall into three
families, each with distinct computational characteristics and memory access
patterns.

\textbf{Convolutional and Dense Networks.}
MobileNet and its variants employ depthwise separable convolutions to reduce
computation while maintaining accuracy, and have been widely deployed on
Cortex-M7 targets via TFLite~Micro.  MCUNet~\cite{lin2020mcunet} introduced
neural architecture search (NAS) specifically constrained to MCU memory
budgets, producing architectures that fit within 256\,KB of SRAM.
MCUNetV2~\cite{lin2021mcunetv2} further reduced peak memory through patch-based
inference, processing input images in spatial tiles.  The depthwise-separable
CNN (DS-CNN) architecture has become the standard for keyword spotting tasks,
achieving over 95\% accuracy on the Google Speech Commands dataset with fewer
than 25K parameters.

\textbf{Recurrent Networks.}
LSTM and GRU networks are essential for temporal sequence processing in
applications such as predictive maintenance, anomaly detection, and voice
activity detection.  Deploying RNNs on MCUs poses unique challenges: the
recurrent state must persist across time steps, gate computations involve
multiple matrix multiplications per step, and the cell state in LSTM is
sensitive to quantization error that accumulates over the sequence length.
FastGRNN~\cite{kusupati2018fastgrnn} addressed the size constraint by
introducing low-rank and sparse gate matrices, reducing model footprint to
kilobyte scale.  However, the inference runtime for FastGRNN was implemented as
a standalone C function rather than an integrated library, and no existing
general-purpose MCU framework provides well-optimized, quantized LSTM and GRU
operators with proper INT16 cell state handling.

\textbf{Transformer Networks.}
The attention mechanism has achieved state-of-the-art results across NLP and
increasingly in vision and time-series tasks.  EdgeBERT~\cite{tambe2021edgebert}
demonstrated that sentence-level energy optimization and early exit strategies
can make BERT-class models feasible on edge processors, though its target
was FPGA accelerators rather than general-purpose MCUs.  Deploying Multi-Head
Attention on Cortex-M cores is challenging due to the quadratic memory
requirement of the attention score matrix ($O(\text{seq}^2)$) and the need for
softmax computation in fixed-point arithmetic.  No existing MCU inference
library provides native Transformer operator support: TFLite~Micro lacks
attention operators, CMSIS-NN provides only DNN kernels, and microTVM requires
model-specific compilation.

This fragmentation means that practitioners deploying multi-architecture
systems---for example, a CNN feature extractor followed by an LSTM temporal
model, or a Transformer encoder for sensor fusion---must integrate multiple
libraries or write custom operator code, with no unified framework for memory
management, quantization, or execution.

% ---------------------------------------------------------------------------
\subsection{Model Partitioning and Distributed Edge Inference}
\label{sec:related_partitioning}

When a single MCU lacks sufficient memory or compute capacity for a complete
model, partitioning the model across multiple devices offers a viable
alternative to model compression.  Kang et al.~\cite{kang2017neurosurgeon}
introduced Neurosurgeon, which dynamically selects the optimal partition point
between a mobile device and the cloud by profiling per-layer latency and data
transfer cost.  Subsequent work extended this idea to edge-only settings where
multiple resource-constrained devices collaborate without cloud connectivity.

Kafetzis and Koutsopoulos~\cite{kafetzis2025wiopt} addressed the specific
problem of DNN partitioning for resource-constrained wireless edge networks,
formulating the partition point selection as an optimization problem that
jointly considers computation latency, communication bandwidth, and per-device
memory constraints.  This work is directly relevant to MCU deployments where
heterogeneous devices (e.g., an STM32MP1 with both Cortex-A7 and Cortex-M4
cores) can split inference across processing elements connected via SPI or
shared memory.  Extending partitioning to multi-layer Transformer architectures
introduces additional complexity, as attention layers exhibit data-dependent
communication volumes (due to variable sequence lengths) and benefit from
speculative execution of subsequent layers while communication is in progress.

Critically, partitioning requires the inference library to support clean layer
boundary semantics: the output tensor of one partition must carry sufficient
metadata (shape, data type, quantization parameters) for the receiving
partition to continue execution without ambiguity.  Existing frameworks do not
explicitly design for this requirement---TFLite~Micro's internal tensor
representation is tightly coupled to its interpreter state, and CMSIS-NN
operates on raw buffers without structured metadata.

% ---------------------------------------------------------------------------
\vspace{0.5em}
\noindent\textbf{Summary.}
Table~\ref{tab:related_comparison} in Section~\ref{sec:evaluation} provides a
detailed feature comparison.  In summary, existing MCU inference tools occupy
distinct but non-overlapping niches: TFLite~Micro provides a graph runtime but
with substantial overhead and limited architecture coverage; CMSIS-NN delivers
optimized kernels but without runtime infrastructure; microTVM offers a
compiler pipeline but with deployment complexity; and vendor tools sacrifice
portability for integration convenience.  No existing library combines
(i)~zero-allocation deterministic inference, (ii)~unified DNN, RNN, and
Transformer operator coverage, (iii)~a quantization-first INT8/INT16 design
with fixed-point requantization, (iv)~a portable HAL with optimized backend
support, and (v)~partitioning-aware tensor descriptors---within a pure C11
implementation under 50\,KB of Flash.  EdgeNN is designed to fill this gap.

% =============================================================================
% Section 2: Related Work (~1.5 pages)
% =============================================================================

\section{Related Work}
\label{sec:related_work}

% ---------------------------------------------------------------------------
\subsection{Inference Frameworks for Microcontrollers}
\label{sec:related_frameworks}

% TFLite Micro: architecture, interpreter model, memory overhead analysis
% CMSIS-NN: operator kernels, integration requirements, what it lacks
% microTVM: compilation approach, deployment complexity
% X-CUBE-AI, Edge Impulse: vendor-specific tools
% TinyEngine, CMix-NN: academic efforts
%
% TODO: Write frameworks subsection

% ---------------------------------------------------------------------------
\subsection{Quantization for MCU Inference}
\label{sec:related_quantization}

% PTQ vs QAT
% Symmetric vs asymmetric schemes
% Per-tensor vs per-channel quantization
% INT8/INT16 hybrid approaches
% Fixed-point arithmetic on Cortex-M (SMMUL, saturating instructions)
%
% TODO: Write quantization subsection

% ---------------------------------------------------------------------------
\subsection{Neural Network Architectures on MCUs}
\label{sec:related_architectures}

% DNNs/CNNs: MobileNet, DS-CNN for keyword spotting, MCUNet
% RNNs: LSTM on MCUs (FastGRNN), challenges of state precision
% Transformers: TinyBERT, DistilBERT, attention on MCUs
% The gap: no single library handles all three families
%
% TODO: Write architectures subsection

% ---------------------------------------------------------------------------
\subsection{Model Partitioning and Distributed Edge Inference}
\label{sec:related_partitioning}

% DNN partitioning literature (Neurosurgeon)
% Your WiOpt 2025 work on dynamic partitioning
% Multi-layer Transformer partitioning with speculative execution
% How EdgeNN's design enables partitioning
%
% TODO: Write partitioning subsection
